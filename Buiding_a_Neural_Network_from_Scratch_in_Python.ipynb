{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s42B4f45SPfa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## downloading the dataset and normalizing images\n",
        "## we normalise images because tensorflow datasets provides images of type tf.uint8, while the model expects tf.float32.\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZRR2qMxSSnq",
        "outputId": "7b011bf1-7b20-460b-8807-72e83f8b999f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import utils\n",
        "num_classes = 10\n",
        "\n",
        "## one hot encoding the data\n",
        "\n",
        "y_train_enc = utils.to_categorical(y_train, num_classes)\n",
        "y_test_enc = utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "8Fzy8spXSmDY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Flattening the data\n",
        "\n",
        "x_train = x_train.reshape(-1,784)\n",
        "x_test = x_test.reshape(-1,784)"
      ],
      "metadata": {
        "id": "uXea4ihzSrEV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BRYG-Z4z-6Lo"
      },
      "outputs": [],
      "source": [
        "class FF_Neural_Net():\n",
        "\n",
        "    \n",
        "\n",
        "    def __init__(self, units, activation='sigmoid'):\n",
        "\n",
        "        ## units: [num_nodes in input layer, num_nodes in hidden layer, num_nodes in output layer]\n",
        "        ## in our case, we will take it to be [784,100,10] : [784 flattened features, 100 hidden units as specified in the question, 10 output features]\n",
        "\n",
        "        self.units = units\n",
        "\n",
        "        #we can later add another activation function, but for now we will go with sigmoid function in hidden layer\n",
        "\n",
        "        self.activation = self.sig  \n",
        "        \n",
        "        # Initialising the weights\n",
        "\n",
        "        self.weights_and_biases = self.initialise()\n",
        "\n",
        "        # Caching all the values after applying activation\n",
        "\n",
        "        self.cache = {}\n",
        "\n",
        "\n",
        "\n",
        "    def sig(self, x, forward=True):\n",
        "\n",
        "      ##applying sigmoid function of y = 1/(1 + exp(-x)) for forward path\n",
        "\n",
        "      if forward:\n",
        "        return 1/(1 + np.exp(-x))\n",
        "\n",
        "      ## applying differential of sigmoid function for back propagation\n",
        "\n",
        "      else:\n",
        "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "\n",
        "      \n",
        "    def softmax(self, x):\n",
        "     \n",
        "     ## SoftMax function for output 'probabilities'\n",
        "     ## Applying the exp(y)/summation(exp(y))\n",
        "\n",
        "        numerator = np.exp(x - x.max())\n",
        "        denominator = np.sum(numerator, axis=0)\n",
        "        return numerator/denominator\n",
        "\n",
        "\n",
        "\n",
        "    def initialise(self):\n",
        "\n",
        "\n",
        "      # num of nodes in each layer\n",
        "\n",
        "\n",
        "      n_x=self.units[0]      ## input layer (our case: 784)\n",
        "\n",
        "      n_hidden=self.units[1]      ## hidden layer (our case: 100)\n",
        "\n",
        "      n_y=self.units[2]      ## output layer (our case: 10)\n",
        "        \n",
        "      weights_and_biases = {\"W1\": np.random.randn(n_hidden, n_x) * np.sqrt(1./n_x),      ## (n_x,n_hidden)\n",
        "                            \"b1\": np.zeros((n_hidden, 1)) * np.sqrt(1./n_x),             ## (1,n_hidden)\n",
        "                            \"W2\": np.random.randn(n_y, n_hidden) * np.sqrt(1./n_hidden), ## (n_hidden, n_y)\n",
        "                            \"b2\": np.zeros((n_y, 1)) * np.sqrt(1./n_hidden)}             ## (1,n_y)\n",
        "      return weights_and_biases\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "    ## y = Weight*X + bias\n",
        "\n",
        "        self.cache[\"X\"] = x\n",
        "\n",
        "        self.cache[\"Z1\"] = np.dot(self.weights_and_biases[\"W1\"], self.cache[\"X\"].T) + self.weights_and_biases[\"b1\"]\n",
        "\n",
        "        self.cache[\"A1\"] = self.activation(self.cache[\"Z1\"])\n",
        "\n",
        "        self.cache[\"Z2\"] = np.dot(self.weights_and_biases[\"W2\"], self.cache[\"A1\"]) + self.weights_and_biases[\"b2\"]\n",
        "\n",
        "        self.cache[\"A2\"] = self.softmax(self.cache[\"Z2\"]) ## using softmax for output layers\n",
        "\n",
        "        return self.cache[\"A2\"]\n",
        "\n",
        "\n",
        "\n",
        "    def backward(self, y, output):\n",
        "\n",
        "    ## using batch gradient descent \n",
        "\n",
        "        current_batch_size = y.shape[0]\n",
        "\n",
        "        ## using differentials for backward propagation\n",
        "\n",
        "        dZ2 = output - y.T\n",
        "        dW2 = (1./current_batch_size) * np.dot(dZ2, self.cache[\"A1\"].T)\n",
        "        db2 = (1./current_batch_size) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        dA1 = np.dot(self.weights_and_biases[\"W2\"].T, dZ2)\n",
        "        dZ1 = dA1 * self.activation(self.cache[\"Z1\"], forward=False) ## applies sig function defined above with the else condition\n",
        "        dW1 = (1./current_batch_size) * np.dot(dZ1, self.cache[\"X\"])\n",
        "        db1 = (1./current_batch_size) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        self.gradients = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
        "\n",
        "        return self.gradients\n",
        "    \n",
        "\n",
        "\n",
        "    def loss(self, y, output):\n",
        "\n",
        "        ## Avg Loss = âˆ’summation(y*log(y_pred))/num_y\n",
        "\n",
        "        l_sum = np.sum(np.multiply(y.T, np.log(output)))\n",
        "\n",
        "        m = y.shape[0]\n",
        "\n",
        "        l = -(1./m) * l_sum\n",
        "\n",
        "        return l\n",
        "     \n",
        "\n",
        "\n",
        "    def optimise(self, learning_rate=0.1, beta=.9):\n",
        " \n",
        "    ## revising weights and biases based on learning rate as multiplier\n",
        "\n",
        "        for i in self.weights_and_biases:\n",
        "          self.weights_and_biases[i] = self.weights_and_biases[i] - learning_rate * self.gradients[i]\n",
        "\n",
        "\n",
        "\n",
        "    def accuracy(self, y, output):\n",
        "        return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, x_train, y_train, x_test, y_test, num_epochs=30, \n",
        "              batch_size=64, learning_rate=0.1, beta=.9):\n",
        "      \n",
        "\n",
        "        # Defining num_epochs and batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        num_batches = -(-x_train.shape[0] // self.batch_size)\n",
        "        \n",
        "        \n",
        "        # Training the network\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "            for j in range(num_batches):\n",
        "              \n",
        "                # Batch\n",
        "\n",
        "                begin = j * self.batch_size\n",
        "                end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
        "\n",
        "                x = x_train[begin:end]\n",
        "                y = y_train[begin:end]\n",
        "                \n",
        "                # Forward\n",
        "\n",
        "                output = self.forward(x)\n",
        "\n",
        "                # Backward propagation\n",
        "\n",
        "                gradients = self.backward(y, output)\n",
        "\n",
        "                # Optimize\n",
        "\n",
        "                self.optimise(learning_rate=learning_rate, beta=beta)\n",
        "\n",
        "            # Evaluate performance on training and test data\n",
        "\n",
        "            # Training data\n",
        "\n",
        "            output = self.forward(x_train)\n",
        "            train_acc = self.accuracy(y_train, output)\n",
        "            train_loss = self.loss(y_train, output)\n",
        "\n",
        "            # Test data\n",
        "\n",
        "            output = self.forward(x_test)\n",
        "            test_acc = self.accuracy(y_test, output)\n",
        "            test_loss = self.loss(y_test, output)\n",
        "\n",
        "\n",
        "            print(f\"Epoch {i+1}:, train acc={train_acc}, train loss={train_loss}, test acc={test_acc}, test loss={test_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ff_nn = FF_Neural_Net(units=[784, 100, 10], activation='sigmoid')\n",
        "model_ff_nn.train(x_train, y_train_enc, x_test, y_test_enc, batch_size=128, learning_rate=0.4, beta=.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw28ZeZ7SzW6",
        "outputId": "e1d50be4-2885-499d-893f-baecc85f2a77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:, train acc=0.8944666666666666, train loss=0.34758745741179775, test acc=0.903, test loss=0.333584849757094\n",
            "Epoch 2:, train acc=0.9162833333333333, train loss=0.2796990552539154, test acc=0.9186, test loss=0.271458833549335\n",
            "Epoch 3:, train acc=0.92825, train loss=0.24094238145658634, test acc=0.9294, test loss=0.23640597139890115\n",
            "Epoch 4:, train acc=0.9379, train loss=0.2126613734462774, test acc=0.9381, test loss=0.21086504388541666\n",
            "Epoch 5:, train acc=0.9444166666666667, train loss=0.19065557311904396, test acc=0.9438, test loss=0.19123255880834092\n",
            "Epoch 6:, train acc=0.94925, train loss=0.1729626963517057, test acc=0.9469, test loss=0.17576393213629002\n",
            "Epoch 7:, train acc=0.9537833333333333, train loss=0.1584312965496865, test acc=0.9503, test loss=0.16333495454778663\n",
            "Epoch 8:, train acc=0.9576666666666667, train loss=0.14627829112967689, test acc=0.9531, test loss=0.15315266479947612\n",
            "Epoch 9:, train acc=0.9610166666666666, train loss=0.13593551045726904, test acc=0.9561, test loss=0.14464826077614548\n",
            "Epoch 10:, train acc=0.96355, train loss=0.12698389105211016, test acc=0.9585, test loss=0.1374128610813183\n",
            "Epoch 11:, train acc=0.9659, train loss=0.11911849822906774, test acc=0.9602, test loss=0.13115590612325856\n",
            "Epoch 12:, train acc=0.9680166666666666, train loss=0.11212058927224808, test acc=0.9618, test loss=0.1256734958494986\n",
            "Epoch 13:, train acc=0.9698, train loss=0.10583264108413612, test acc=0.9633, test loss=0.12082205083026937\n",
            "Epoch 14:, train acc=0.9716666666666667, train loss=0.10013879430052468, test acc=0.9645, test loss=0.11649744385501386\n",
            "Epoch 15:, train acc=0.9732333333333333, train loss=0.09495158242253977, test acc=0.9659, test loss=0.11262098595615216\n",
            "Epoch 16:, train acc=0.97455, train loss=0.09020334566130217, test acc=0.9673, test loss=0.10913114770401117\n",
            "Epoch 17:, train acc=0.97595, train loss=0.08584035532045993, test acc=0.9678, test loss=0.10597828023145876\n",
            "Epoch 18:, train acc=0.9772333333333333, train loss=0.08181854876130468, test acc=0.9684, test loss=0.10312095322259072\n",
            "Epoch 19:, train acc=0.97855, train loss=0.07810058545959214, test acc=0.9694, test loss=0.10052364561151349\n",
            "Epoch 20:, train acc=0.97975, train loss=0.0746540677657164, test acc=0.9694, test loss=0.09815552030869601\n",
            "Epoch 21:, train acc=0.9804333333333334, train loss=0.07145057467389934, test acc=0.9702, test loss=0.09598983970549355\n",
            "Epoch 22:, train acc=0.9811666666666666, train loss=0.06846510788140159, test acc=0.9709, test loss=0.09400361608621811\n",
            "Epoch 23:, train acc=0.98195, train loss=0.065675686801414, test acc=0.9713, test loss=0.09217725241328746\n",
            "Epoch 24:, train acc=0.9827833333333333, train loss=0.06306298729302914, test acc=0.9716, test loss=0.09049408608829797\n",
            "Epoch 25:, train acc=0.9836, train loss=0.06061000725652995, test acc=0.9722, test loss=0.08893985845855322\n",
            "Epoch 26:, train acc=0.9843333333333333, train loss=0.05830176395381687, test acc=0.9727, test loss=0.08750219217089424\n",
            "Epoch 27:, train acc=0.9848833333333333, train loss=0.05612502431872961, test acc=0.973, test loss=0.08617016404794575\n",
            "Epoch 28:, train acc=0.9856, train loss=0.05406807048585289, test acc=0.9732, test loss=0.08493402041115732\n",
            "Epoch 29:, train acc=0.9861666666666666, train loss=0.052120504825557024, test acc=0.9734, test loss=0.08378502246616647\n",
            "Epoch 30:, train acc=0.9867833333333333, train loss=0.050273092337748404, test acc=0.9737, test loss=0.08271536840771967\n"
          ]
        }
      ]
    }
  ]
}